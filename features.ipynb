{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dda2889",
   "metadata": {},
   "source": [
    "## Import general Python libraries, LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07b00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.9/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.9/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.9/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.9/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: liwc in ./venv/lib/python3.9/site-packages (0.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install nltk\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "%pip install liwc\n",
    "import liwc\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028081b",
   "metadata": {},
   "source": [
    "## Import nltk libraries and download packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82206b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97806471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/danakurniawan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369934a",
   "metadata": {},
   "source": [
    "## Text features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2beed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 23540\n",
      "Columns: ['dialog_id', 'speaker', 'transcript', 'da_tag', 'start_time', 'end_time', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler']\n",
      "\n",
      "First few transcript values:\n",
      "0                                   SIL\n",
      "1                                   SIL\n",
      "2            uh do you have a pet randy\n",
      "3                                   SIL\n",
      "4                               uh yeah\n",
      "5            currently we have a poodle\n",
      "6                                   SIL\n",
      "7    a poodle miniature or uh full size\n",
      "8                                  yeah\n",
      "9                                   SIL\n",
      "Name: transcript, dtype: object\n",
      "\n",
      " extracting features from every row in the set...\n",
      "Processing row 0/23540...\n",
      "Processing row 1000/23540...\n",
      "Processing row 2000/23540...\n",
      "Processing row 3000/23540...\n",
      "Processing row 4000/23540...\n",
      "Processing row 5000/23540...\n",
      "Processing row 6000/23540...\n",
      "Processing row 7000/23540...\n",
      "Processing row 8000/23540...\n",
      "Processing row 9000/23540...\n",
      "Processing row 10000/23540...\n",
      "Processing row 11000/23540...\n",
      "Processing row 12000/23540...\n",
      "Processing row 13000/23540...\n",
      "Processing row 14000/23540...\n",
      "Processing row 15000/23540...\n",
      "Processing row 16000/23540...\n",
      "Processing row 17000/23540...\n",
      "Processing row 18000/23540...\n",
      "Processing row 19000/23540...\n",
      "Processing row 20000/23540...\n",
      "Processing row 21000/23540...\n",
      "Processing row 22000/23540...\n",
      "Processing row 23000/23540...\n",
      "Finished processing all 23540 rows\n",
      "\n",
      "============================================================\n",
      "processing done for NLTK features\n",
      "============================================================\n",
      "Original shape: (23540, 79)\n",
      "New shape (first 2 columns dropped): (23540, 106)\n",
      "Rows processed: 23540\n",
      "\n",
      "NLTK features extracted (29 total):\n",
      "['word_count', 'sentence_count', 'avg_sentence_length', 'noun_count', 'proper_noun_count', 'verb_count', 'modal_verb_count', 'adj_count', 'adv_count', 'pronoun_count', 'determiner_count', 'preposition_count', 'conjunction_count', 'interjection_count', 'particle_count', 'pos_tag_diversity', 'named_entity_count', 'person_entity_count', 'organization_entity_count', 'location_entity_count', 'stopword_count', 'stopword_ratio', 'unique_bigrams', 'unique_trigrams', 'bigram_diversity', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_compound']\n",
      "\n",
      "First few rows with key features:\n",
      "\n",
      "Columns in final dataset:\n",
      "['transcript', 'da_tag', 'start_time', 'end_time', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'word_count', 'sentence_count', 'avg_sentence_length', 'noun_count', 'proper_noun_count', 'verb_count', 'modal_verb_count', 'adj_count', 'adv_count', 'pronoun_count', 'determiner_count', 'preposition_count', 'conjunction_count', 'interjection_count', 'particle_count', 'pos_tag_diversity', 'named_entity_count', 'person_entity_count', 'organization_entity_count', 'location_entity_count', 'stopword_count', 'stopword_ratio', 'unique_bigrams', 'unique_trigrams', 'bigram_diversity', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_compound']\n",
      "nltk_features_df shape: (23540, 29)\n",
      "nltk_features_df columns: ['word_count', 'sentence_count', 'avg_sentence_length', 'noun_count', 'proper_noun_count', 'verb_count', 'modal_verb_count', 'adj_count', 'adv_count', 'pronoun_count', 'determiner_count', 'preposition_count', 'conjunction_count', 'interjection_count', 'particle_count', 'pos_tag_diversity', 'named_entity_count', 'person_entity_count', 'organization_entity_count', 'location_entity_count', 'stopword_count', 'stopword_ratio', 'unique_bigrams', 'unique_trigrams', 'bigram_diversity', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_compound']\n",
      "\n",
      "First few rows:\n",
      "                            transcript  da_tag  start_time  end_time  \\\n",
      "0                                  SIL     NaN     0.00000   2.92922   \n",
      "1                                  SIL     NaN     0.00000   4.46754   \n",
      "2           uh do you have a pet randy     NaN     2.92922   4.31867   \n",
      "3                                  SIL     NaN     4.31867   6.74838   \n",
      "4                              uh yeah     NaN     4.46754   5.01339   \n",
      "5           currently we have a poodle     NaN     5.01339   6.46325   \n",
      "6                                  SIL     NaN     6.46325   7.71784   \n",
      "7   a poodle miniature or uh full size     NaN     6.74838   9.40000   \n",
      "8                                 yeah     NaN     7.71784   8.18925   \n",
      "9                                  SIL     NaN     8.18925   9.38020   \n",
      "10                uh it's uh miniature     NaN     9.38020  10.89370   \n",
      "11                                 SIL     NaN     9.40000  11.06463   \n",
      "12                                 SIL     NaN    10.89370  12.05984   \n",
      "13                              uh-huh     NaN    11.06463  12.55607   \n",
      "14                                yeah     NaN    12.05984  12.50288   \n",
      "\n",
      "    function   pronoun     ppron    i   we       you  ...  \\\n",
      "0   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "1   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "2   0.571429  0.142857  0.142857  0.0  0.0  0.142857  ...   \n",
      "3   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "4   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "5   0.600000  0.200000  0.200000  0.0  0.2  0.000000  ...   \n",
      "6   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "7   0.285714  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "8   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "9   0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "10  0.250000  0.250000  0.000000  0.0  0.0  0.000000  ...   \n",
      "11  0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "12  0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "13  0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "14  0.000000  0.000000  0.000000  0.0  0.0  0.000000  ...   \n",
      "\n",
      "    location_entity_count  stopword_count  stopword_ratio  unique_bigrams  \\\n",
      "0                       0               0        0.000000               0   \n",
      "1                       0               4        0.571429               6   \n",
      "2                       0               4        0.571429               6   \n",
      "3                       0               3        0.600000               4   \n",
      "4                       0               3        0.600000               4   \n",
      "5                       0               3        0.600000               4   \n",
      "6                       0               0        0.000000               0   \n",
      "7                       0               0        0.000000               0   \n",
      "8                       0               0        0.000000               0   \n",
      "9                       0               1        0.200000               4   \n",
      "10                      0               1        0.200000               4   \n",
      "11                      0               0        0.000000               0   \n",
      "12                      0               0        0.000000               0   \n",
      "13                      0               0        0.000000               0   \n",
      "14                      0               0        0.000000               0   \n",
      "\n",
      "    unique_trigrams  bigram_diversity  sentiment_positive  sentiment_negative  \\\n",
      "0                 0               0.0                   0                   0   \n",
      "1                 5               1.0                   0                   0   \n",
      "2                 5               1.0                   0                   0   \n",
      "3                 3               1.0                   0                   0   \n",
      "4                 3               1.0                   0                   0   \n",
      "5                 3               1.0                   0                   0   \n",
      "6                 0               0.0                   0                   0   \n",
      "7                 0               0.0                   0                   0   \n",
      "8                 0               0.0                   0                   0   \n",
      "9                 3               1.0                   0                   0   \n",
      "10                3               1.0                   0                   0   \n",
      "11                0               0.0                   0                   0   \n",
      "12                0               0.0                   0                   0   \n",
      "13                0               0.0                   0                   0   \n",
      "14                0               0.0                   0                   0   \n",
      "\n",
      "    sentiment_neutral  sentiment_compound  \n",
      "0                   0                   0  \n",
      "1                   0                   0  \n",
      "2                   0                   0  \n",
      "3                   0                   0  \n",
      "4                   0                   0  \n",
      "5                   0                   0  \n",
      "6                   0                   0  \n",
      "7                   0                   0  \n",
      "8                   0                   0  \n",
      "9                   0                   0  \n",
      "10                  0                   0  \n",
      "11                  0                   0  \n",
      "12                  0                   0  \n",
      "13                  0                   0  \n",
      "14                  0                   0  \n",
      "\n",
      "[15 rows x 106 columns]\n",
      "\n",
      " check for any missing values in extracted features:\n",
      "0 missing values found\n",
      "\n",
      " saved to: text_features_test.csv\n"
     ]
    }
   ],
   "source": [
    "# df_train = pd.read_csv(\"/Users/danakurniawan/Documents/Columbia/COMS_6706/hw2/train.csv\")\n",
    "# df_val = pd.read_csv(\"/Users/danakurniawan/Documents/Columbia/COMS_6706/hw2/valid.csv\") \n",
    "df_test = pd.read_csv(\"/Users/danakurniawan/Documents/Columbia/COMS_6706/hw2/test.csv\")\n",
    "\n",
    "#check that shape is still the same\n",
    "print(f\"Total rows in dataset: {len(df_test)}\")\n",
    "print(f\"Columns: {df_test.columns.tolist()}\")\n",
    "print(f\"\\nFirst few transcript values:\")\n",
    "print(df_test['transcript'].head(10))\n",
    "\n",
    "def extract_nltk_features(text):\n",
    "\n",
    "    \"\"\"Extract NLTK features from train.csv for text-based set\"\"\"\n",
    "    if pd.isna(text) or text == 'SIL':\n",
    "        return {\n",
    "            #tokenization\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "\n",
    "            #parts of speech tags and categories\n",
    "            'noun_count': 0,\n",
    "            'proper_noun_count': 0,\n",
    "            'verb_count': 0,\n",
    "            'modal_verb_count': 0,\n",
    "            'adj_count': 0,\n",
    "            'adv_count': 0,\n",
    "            'pronoun_count': 0,\n",
    "            'determiner_count': 0,\n",
    "            'preposition_count': 0,\n",
    "            'conjunction_count': 0,\n",
    "            'interjection_count': 0,\n",
    "            'particle_count': 0,\n",
    "            'pos_tag_diversity': 0,\n",
    "\n",
    "            # Named Entity Recognition\n",
    "            'named_entity_count': 0,\n",
    "            'person_entity_count': 0,\n",
    "            'organization_entity_count': 0,\n",
    "            'location_entity_count': 0,\n",
    "            \n",
    "            # Stopwords\n",
    "            'stopword_count': 0,\n",
    "            'stopword_ratio': 0,\n",
    "            \n",
    "            # N-gram features\n",
    "            'unique_bigrams': 0,\n",
    "            'unique_trigrams': 0,\n",
    "            'bigram_diversity': 0,\n",
    "            \n",
    "            # Sentiment (VADER from NLTK)\n",
    "            'sentiment_positive': 0,\n",
    "            'sentiment_negative': 0,\n",
    "            'sentiment_neutral': 0,\n",
    "            'sentiment_compound': 0,\n",
    "        }\n",
    "    \n",
    "    #normalize the text and create the feature set\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_original = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    features['word_count'] = len(tokens)\n",
    "    features['sentence_count'] = len(sentences)\n",
    "    features['avg_sentence_length'] = len(tokens) / len(sentences) if sentences else 0\n",
    "    \n",
    "    #pos features\n",
    "    pos_tags = pos_tag(tokens_original)\n",
    "    pos_tag_list = [tag for word, tag in pos_tags]\n",
    "\n",
    "    #count different POS categories\n",
    "    features['noun_count'] = sum(1 for tag in pos_tag_list if tag.startswith('NN'))\n",
    "    features['proper_noun_count'] = sum(1 for tag in pos_tag_list if tag in ['NNP', 'NNPS'])\n",
    "    features['verb_count'] = sum(1 for tag in pos_tag_list if tag.startswith('VB'))\n",
    "    features['modal_verb_count'] = sum(1 for tag in pos_tag_list if tag == 'MD')\n",
    "    features['adj_count'] = sum(1 for tag in pos_tag_list if tag.startswith('JJ'))\n",
    "    features['adv_count'] = sum(1 for tag in pos_tag_list if tag.startswith('RB'))\n",
    "    features['pronoun_count'] = sum(1 for tag in pos_tag_list if tag.startswith('PRP'))\n",
    "    features['determiner_count'] = sum(1 for tag in pos_tag_list if tag == 'DT')\n",
    "    features['preposition_count'] = sum(1 for tag in pos_tag_list if tag == 'IN')\n",
    "    features['conjunction_count'] = sum(1 for tag in pos_tag_list if tag in ['CC', 'IN'])\n",
    "    features['interjection_count'] = sum(1 for tag in pos_tag_list if tag == 'UH')\n",
    "    features['particle_count'] = sum(1 for tag in pos_tag_list if tag == 'RP')\n",
    "\n",
    "    # POS tag diversity\n",
    "    unique_pos_tags = len(set(pos_tag_list))\n",
    "    features['pos_tag_diversity'] = unique_pos_tags / len(pos_tag_list) if pos_tag_list else 0\n",
    "    \n",
    "    #named entity recognition (NER)\n",
    "    try:\n",
    "        tree = ne_chunk(pos_tags, binary=False)\n",
    "        entities = []\n",
    "        person_count = 0\n",
    "        org_count = 0\n",
    "        location_count = 0\n",
    "        \n",
    "        for subtree in tree:\n",
    "            if hasattr(subtree, 'label'):\n",
    "                entity_label = subtree.label()\n",
    "                entities.append(entity_label)\n",
    "                if entity_label == 'PERSON':\n",
    "                    person_count += 1\n",
    "                elif entity_label == 'ORGANIZATION':\n",
    "                    org_count += 1\n",
    "                elif entity_label in ['GPE', 'LOCATION']:\n",
    "                    location_count += 1\n",
    "        \n",
    "        features['named_entity_count'] = len(entities)\n",
    "        features['person_entity_count'] = person_count\n",
    "        features['organization_entity_count'] = org_count\n",
    "        features['location_entity_count'] = location_count\n",
    "    except:\n",
    "        features['named_entity_count'] = 0\n",
    "        features['person_entity_count'] = 0\n",
    "        features['organization_entity_count'] = 0\n",
    "        features['location_entity_count'] = 0\n",
    "    \n",
    "    #get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopword_count = sum(1 for word in tokens if word in stop_words)\n",
    "    features['stopword_count'] = stopword_count\n",
    "    features['stopword_ratio'] = stopword_count / len(tokens) if tokens else 0\n",
    "    \n",
    "    #get n_grams \n",
    "    if len(tokens) >= 2:\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        features['unique_bigrams'] = len(set(bigrams_list))\n",
    "        features['bigram_diversity'] = len(set(bigrams_list)) / len(bigrams_list) if bigrams_list else 0\n",
    "    else:\n",
    "        features['unique_bigrams'] = 0\n",
    "        features['bigram_diversity'] = 0\n",
    "    \n",
    "    if len(tokens) >= 3:\n",
    "        trigrams_list = list(ngrams(tokens, 3))\n",
    "        features['unique_trigrams'] = len(set(trigrams_list))\n",
    "    else:\n",
    "        features['unique_trigrams'] = 0\n",
    "\n",
    "    #sentiment analysis\n",
    "    try:\n",
    "        sentiment_scores = sia.polarity_scores(text)\n",
    "        features['sentiment_positive'] = sentiment_scores['pos']\n",
    "        features['sentiment_negative'] = sentiment_scores['neg']\n",
    "        features['sentiment_neutral'] = sentiment_scores['neu']\n",
    "        features['sentiment_compound'] = sentiment_scores['compound']\n",
    "    except:\n",
    "        features['sentiment_positive'] = 0\n",
    "        features['sentiment_negative'] = 0\n",
    "        features['sentiment_neutral'] = 0\n",
    "        features['sentiment_compound'] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "#extract NLTK features from transcript\n",
    "print(\"\\n extracting features from every row in the set...\") \n",
    "nltk_features_list = []\n",
    "\n",
    "for idx, text in enumerate(df_test['transcript']):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processing row {idx}/{len(df_test)}...\")\n",
    "    features = extract_nltk_features(text)\n",
    "    nltk_features_list.append(features)\n",
    "\n",
    "print(f\"Finished processing all {len(nltk_features_list)} rows\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "nltk_features_df = pd.DataFrame(nltk_features_list)\n",
    "\n",
    "# Keep only the original columns (all columns from loaded CSV)\n",
    "# Since we want to drop dialog_id and speaker, let's explicitly select what we want\n",
    "original_columns = [col for col in df_test.columns if col not in ['dialog_id', 'speaker']]\n",
    "df_test_selected = df_test[original_columns].copy()\n",
    "\n",
    "# Combine with NLTK features\n",
    "df_test_with_nltk = pd.concat([df_test_selected.reset_index(drop=True), \n",
    "                             nltk_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the enhanced dataset\n",
    "df_test_with_nltk.to_csv('text_features_test.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"processing done for NLTK features\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original shape: {df_test.shape}\")\n",
    "print(f\"New shape (first 2 columns dropped): {df_test_with_nltk.shape}\")\n",
    "print(f\"Rows processed: {len(nltk_features_list)}\")\n",
    "print(f\"\\nNLTK features extracted ({len(nltk_features_df.columns)} total):\")\n",
    "print(list(nltk_features_df.columns)) \n",
    "\n",
    "print(f\"\\nFirst few rows with key features:\")\n",
    "# Check what columns actually exist\n",
    "print(\"\\nColumns in final dataset:\")\n",
    "print(df_test_with_nltk.columns.tolist())\n",
    "\n",
    "print(\"nltk_features_df shape:\", nltk_features_df.shape)\n",
    "print(\"nltk_features_df columns:\", nltk_features_df.columns.tolist())\n",
    "\n",
    "# Print first few rows of any columns that exist\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_test_with_nltk.head(15))\n",
    "\n",
    "print(f\"\\n check for any missing values in extracted features:\")\n",
    "print(nltk_features_df.isnull().sum().sum(), \"missing values found\")\n",
    "\n",
    "print(f\"\\n saved to: text_features_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df311c7c",
   "metadata": {},
   "source": [
    "# Speech features extraction  \n",
    "## (also available in speech_features.py due to running with asyncio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d087add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import librosa\n",
    "from librosa.feature import mfcc, rms, zero_crossing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomProcessor():\n",
    "    def __init__(self):\n",
    "        # self.processor_rosa = librosa\n",
    "        # self.processor_praat = None\n",
    "        # self.sr = sampling_rate\n",
    "        pass\n",
    "\n",
    "    def __call__(self, y, custom_sr):  \n",
    "\n",
    "        # print(\"IN call\", end=\"\")\n",
    "        output = np.zeros(45) #initialize for all features\n",
    "        # print(y.shape)\n",
    "        # print(rms(y=y, frame_length=y.size))\n",
    "\n",
    "        #handle short utterances by restricting?\n",
    "        frame_len = len(y) if len(y) > 0 else 512\n",
    "        n_fft_size = len(y) if len(y) > 0 else 512\n",
    "\n",
    "        #rms for amplitude\n",
    "        rms_values = rms(y=y, frame_length=y.size)[0]\n",
    "        output[0] = np.mean(rms_values) #mean\n",
    "        output[1] = np.std(rms_values) #std dev \n",
    "        output[2] = np.max(rms_values) #max\n",
    "        output[3] = np.min(rms_values) #min\n",
    "\n",
    "        #zero crossing rate for crossing above y axis used to separate eg. consonants, vowels \n",
    "        zcr = zero_crossing_rate(y, frame_length=frame_len)[0]\n",
    "        output[4] = np.mean(zcr)\n",
    "        output[5] = np.std(zcr)\n",
    "\n",
    "        # Decided not to factor in spectral centroid - too expensive to compute (>20 hours)\n",
    "        #spectral_centroid\n",
    "        # spectral_centroid_val = np.squeeze(spectral_centroid(y=y, n_fft=n_fft_size)) # Use squeeze instead of indexing the first one\n",
    "        # print(spectral_centroid(y=y, n_fft=n_fft_size))\n",
    "        # # This is where function termination happened previously, because this always caused an error (because it was calling mean() on a function, not an array)\n",
    "        # output[6] = np.mean(spectral_centroid_val)\n",
    "        # output[7] = np.std(spectral_centroid_val)\n",
    "\n",
    "        # #mfcc - mel frequency values\n",
    "        # mfcc_features = mfcc(y=y, sr=custom_sr, n_mfcc=13, n_fft=n_fft_size)\n",
    "        # for i in range(13):\n",
    "        #     output[8 + i] = np.mean(mfcc_features[i]) # MFCC means at [indices] 8-20\n",
    "        #     output[8 + 13 + i] = np.std(mfcc_features[i]) #MFCC stds at [indices] 21-33\n",
    "    \n",
    "        #pitch and magnitude \n",
    "        pitches, magnitudes = librosa.piptrack(y=y, n_fft=n_fft_size)\n",
    "        \n",
    "        # Extract pitch values where magnitude is high\n",
    "        pitch_vals = []\n",
    "        for t in range(pitches.shape[1]):\n",
    "            index = magnitudes[:, t].argmax()\n",
    "            pitch = pitches[index, t]\n",
    "            if pitch > 0:\n",
    "                pitch_vals.append(pitch)\n",
    "        \n",
    "        if len(pitch_vals) > 0:\n",
    "            output[34] = np.mean(pitch_vals)\n",
    "            output[35] = np.std(pitch_vals)\n",
    "            output[36] = np.max(pitch_vals)\n",
    "            output[37] = np.min(pitch_vals)\n",
    "            output[38] = np.max(pitch_vals) - np.min(pitch_vals)  # pitch range\n",
    "        \n",
    "        # duration\n",
    "        output[39] = len(y) / 22050  # sr = 22050 Duration in seconds\n",
    "        output[40] = librosa.feature.tempo(y=y, sr=custom_sr)[0] if len(y) > custom_sr else 0\n",
    "        # print(\" END call\")\n",
    "             \n",
    "        return output\n",
    "\n",
    "    # #in case of errors output zeros\n",
    "    # def _return_zeros(self):\n",
    "    #     \"\"\"Return zero features for failed extractions (kept for compatibility)\"\"\"\n",
    "    #     features = {\n",
    "    #         'rms_mean': 0, 'rms_std': 0, 'rms_max': 0, 'rms_min': 0,\n",
    "    #         'zcr_mean': 0, 'zcr_std': 0,\n",
    "    #         'spectral_centroid_mean': 0, 'spectral_centroid_std': 0,\n",
    "    #         'pitch_mean': 0, 'pitch_std': 0, 'pitch_max': 0, 'pitch_min': 0, 'pitch_range': 0,\n",
    "    #         'duration': 0, 'tempo': 0, \n",
    "    #     }\n",
    "    #     # Add MFCC zeros\n",
    "    #     for i in range(13):\n",
    "    #         features[f'mfcc_{i}_mean'] = 0\n",
    "    #         features[f'mfcc_{i}_std'] = 0\n",
    "    #     return features   \n",
    "\n",
    "\n",
    "def process_row(\n",
    "    row:pd.Series,\n",
    "    processor:CustomProcessor,\n",
    "    base_path: str, #add base path\n",
    "    progress_callback:Optional[callable] = None,\n",
    "    ):\n",
    "    \n",
    "    # Get the information specifying where to calculate the speech information\n",
    "    dialog_id = row[\"dialog_id\"]\n",
    "    speaker = row[\"speaker\"]\n",
    "    start_time = row[\"start_time\"]\n",
    "    end_time = row[\"end_time\"]\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    file_name = f\"{dialog_id}_{speaker}.wav\"\n",
    "    file_path = os.path.join(base_path, \"wav\", file_name)\n",
    "    \n",
    "    #load audio segment \n",
    "    print(f\"Loading sample {dialog_id} beginning at time {start_time}\")\n",
    "    # y, sample_rate = librosa.load(file_path, sr=None, offset=start_time, duration=duration)\n",
    "    y, sample_rate = librosa.load(file_path, offset=start_time, duration=duration)\n",
    "\n",
    "    # # ret = processor(y, sr=sample_rate)  # Didn't define the processor to take sr, probably not needed either way\n",
    "    ret = processor(y, sample_rate)\n",
    "    print(f\"Finished processing sample at time {start_time}\")\n",
    "\n",
    "    if progress_callback:\n",
    "        progress_callback()\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "num_cores = 20\n",
    "sema = asyncio.Semaphore(num_cores)\n",
    "\n",
    "async def process_row_v2(\n",
    "    row:pd.Series,\n",
    "    processor:CustomProcessor,\n",
    "    base_path: str, #add base path\n",
    "    progress_callback:Optional[callable] = None,\n",
    "    ):\n",
    "\n",
    "    async with sema:\n",
    "    \n",
    "        # Get the information specifying where to calculate the speech information\n",
    "        dialog_id = row[\"dialog_id\"]\n",
    "        speaker = row[\"speaker\"]\n",
    "        start_time = row[\"start_time\"]\n",
    "        end_time = row[\"end_time\"]\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        file_name = f\"{dialog_id}_{speaker}.wav\"\n",
    "        file_path = os.path.join(base_path, \"wav\", file_name)\n",
    "        \n",
    "        #load audio segment\n",
    "        # sample_rate = 22050 ?? \n",
    "        # print(f\"Loading sample {dialog_id} beginning at time {start_time}\")\n",
    "        # y, sample_rate = librosa.load(file_path, sr=None, offset=start_time, duration=duration)\n",
    "        y, sample_rate = librosa.load(file_path, offset=start_time, duration=duration)\n",
    "\n",
    "        # # ret = processor(y, sr=sample_rate)  # Didn't define the processor to take sr, probably not needed either way\n",
    "        ret = processor(y, sample_rate)\n",
    "        # print(f\"Finished processing sample at time {start_time}\")\n",
    "\n",
    "        if progress_callback:\n",
    "            progress_callback()\n",
    "\n",
    "        return ret\n",
    "\n",
    "async def process_rows(\n",
    "    rows:pd.DataFrame,\n",
    "    processor:CustomProcessor,\n",
    "    base_path: str, #add path\n",
    "    ) -> list[np.ndarray]:\n",
    "\n",
    "    prog = tqdm(total=rows.shape[0])\n",
    "    # num_cores = 1\n",
    "    # sema = asyncio.Semaphore(num_cores)\n",
    "    # async with sema:\n",
    "\n",
    "    # results = await asyncio.gather(\n",
    "    #     *[\n",
    "    #         # asyncio.to_thread(process_row, row, processor, base_path, prog.update) for _, row in rows.iterrows()\n",
    "    #         asyncio.to_thread(process_row, row, processor, base_path, prog.update) for _, row in rows.iterrows()\n",
    "    #         # asyncio.to_thread(process_wav, row, processor, prog.update) for _, row in rows.iterrows()\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # num_cores = 16\n",
    "    # sema = asyncio.Semaphore(num_cores)\n",
    "    # async with sema:\n",
    "\n",
    "    tasks = [\n",
    "        process_row_v2(row, processor, base_path, prog.update)\n",
    "        for _, row in rows.iterrows()\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    prog.close()\n",
    "    return results\n",
    "    # prog.close() \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = CustomProcessor()\n",
    "    # base_path = \"/Users/danakurniawan/Documents/Columbia/COMS_6706/hw2\"\n",
    "    base_path = \".\"\n",
    "\n",
    "    # metadata = pd.read_csv(os.path.join(base_path, \"train.csv\")) #Create speech train set\n",
    "    # metadata = pd.read_csv(os.path.join(base_path, \"valid.csv\")) #Create speech validation set\n",
    "    metadata = pd.read_csv(os.path.join(base_path, \"test.csv\")) #Create speech test set\n",
    "    print(\"finished loading metadata\")\n",
    "\n",
    "    # test on a single row \n",
    "    print(\"\\n testing on the first row\")\n",
    "    test_row = metadata.iloc[1]\n",
    "    test_process_on_row = process_row(test_row, processor, base_path)\n",
    "\n",
    "    # # features = asyncio.run(process_rows(trunc_metadata, processor, base_path))\n",
    "\n",
    "    # # Try running with semaphore to limit number of threads\n",
    "    # features = asyncio.run(process_rows(trunc_metadata, processor, base_path))\n",
    "\n",
    "    # extract features asynchronously across rows in a version like this\n",
    "    features = asyncio.run(process_rows(metadata, processor, base_path)) #features is a list of arrays\n",
    "    print(\"finished extracting speech features\")\n",
    "\n",
    "    # # TODO: save, or run with -i to not lose the computation at the end\n",
    "    saved_features = np.array(features)\n",
    "    np.save(\"speech_features_test.npy\", saved_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f97e9",
   "metadata": {},
   "source": [
    "## convert to .csv from .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .npy file\n",
    "input_file = 'speech_features_train.npy'\n",
    "data = np.load(input_file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# save training to CSV\n",
    "output_file = 'speech_features_train.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\saved to {output_file}\")\n",
    "print(f\"CSV file contains {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "#validation\n",
    "input_file_valid = 'speech_features_valid.npy'\n",
    "data_validation = np.load(input_file_valid)\n",
    "df_validation = pd.DataFrame(data_validation)\n",
    "output_file_validation = 'speech_features_valid.csv'\n",
    "df_validation.to_csv(output_file_validation, index=False)\n",
    "print(f\"\\saved to {output_file_validation}\")\n",
    "print(f\"CSV file contains {df_validation.shape[0]} rows and {df_validation.shape[1]} columns\")\n",
    "\n",
    "#test\n",
    "input_file_test = 'speech_features_test.npy'\n",
    "data_test = np.load(input_file_test)\n",
    "df_test = pd.DataFrame(data_test)\n",
    "output_file_test = 'speech_features_test.csv'\n",
    "df_test.to_csv(output_file_test, index=False)\n",
    "print(f\"\\saved to {output_file_test}\")\n",
    "print(f\"CSV file contains {df_test.shape[0]} rows and {df_test.shape[1]} columns\")\n",
    "\n",
    "# Print information about the data\n",
    "print(f\"Loaded array shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(data[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
